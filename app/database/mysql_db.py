# app/database/mysql_db.py
import mysql.connector
from mysql.connector import Error
import json
import logging
from datetime import datetime
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import joblib
import os

logger = logging.getLogger(__name__)

class MySQLTextDB:
    def __init__(self, config):
        self.config = config
        self.connection = None
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é TfidfVectorizer
        self.vectorizer = TfidfVectorizer(
            max_features=1000, 
            stop_words=list(russian_stop_words())  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫
        )
        
        self.tfidf_matrix = None
        self.document_ids = []
        self._connect()
        self._create_tables()
        self._load_tfidf_model()

    def _connect(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ MySQL"""
        try:
            self.connection = mysql.connector.connect(**self.config)
            logger.info("‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ MySQL")
        except Error as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ MySQL: {e}")
            raise

    def _create_tables(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç"""
        try:
            cursor = self.connection.cursor()

            create_documents_table = """
            CREATE TABLE IF NOT EXISTS documents (
                id INT AUTO_INCREMENT PRIMARY KEY,
                content TEXT NOT NULL,
                source VARCHAR(500),
                type VARCHAR(50),
                chunk_index INT DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                keywords TEXT,
                INDEX idx_source (source),
                INDEX idx_type (type),
                FULLTEXT idx_content (content)
            )
            """

            cursor.execute(create_documents_table)
            self.connection.commit()
            cursor.close()
            logger.info("‚úÖ –¢–∞–±–ª–∏—Ü—ã —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ")

        except Error as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü: {e}")
            raise

    def _load_tfidf_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ TF-IDF –º–æ–¥–µ–ª–∏"""
        try:
            if os.path.exists('tfidf_model.pkl'):
                self.vectorizer = joblib.load('tfidf_model.pkl')
                self.tfidf_matrix = joblib.load('tfidf_matrix.pkl')
                self.document_ids = joblib.load('document_ids.pkl')
                logger.info("‚úÖ TF-IDF –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å TF-IDF –º–æ–¥–µ–ª—å: {e}")

    def _save_tfidf_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ TF-IDF –º–æ–¥–µ–ª–∏"""
        try:
            joblib.dump(self.vectorizer, 'tfidf_model.pkl')
            joblib.dump(self.tfidf_matrix, 'tfidf_matrix.pkl')
            joblib.dump(self.document_ids, 'document_ids.pkl')
            logger.info("‚úÖ TF-IDF –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è TF-IDF –º–æ–¥–µ–ª–∏: {e}")

    def _preprocess_text(self, text):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _extract_keywords(self, text, top_n=10):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        words = re.findall(r'\b[–∞-—è—ë]{3,}\b', text.lower())
        word_freq = {}
        for word in words:
            if word not in russian_stop_words():
                word_freq[word] = word_freq.get(word, 0) + 1
        
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return ' '.join([word for word, freq in sorted_words[:top_n]])

    def store_documents(self, documents):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É"""
        try:
            cursor = self.connection.cursor()

            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
            cursor.execute("DELETE FROM documents")
            logger.info("üóëÔ∏è –û—á–∏—â–µ–Ω—ã —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ")

            stored_count = 0
            all_texts = []

            for i, doc in enumerate(documents):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                keywords = self._extract_keywords(doc['content'])
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
                insert_doc_query = """
                INSERT INTO documents (content, source, type, chunk_index, keywords)
                VALUES (%s, %s, %s, %s, %s)
                """
                cursor.execute(insert_doc_query, (
                    doc['content'],
                    doc.get('source', 'unknown'),
                    doc.get('type', 'website'),
                    doc.get('chunk_index', 0),
                    keywords
                ))

                document_id = cursor.lastrowid
                stored_count += 1
                all_texts.append(self._preprocess_text(doc['content']))

            self.connection.commit()
            cursor.close()

            # –û–±—É—á–∞–µ–º TF-IDF –º–æ–¥–µ–ª—å
            if all_texts:
                self.tfidf_matrix = self.vectorizer.fit_transform(all_texts)
                self.document_ids = list(range(1, stored_count + 1))
                self._save_tfidf_model()

            logger.info(f"üíæ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stored_count}")
            return stored_count

        except Error as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            self.connection.rollback()
            raise

    def search_similar_documents(self, query, k=3):
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É"""
        try:
            if self.tfidf_matrix is None or not self.document_ids:
                # Fallback: –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                return self._keyword_search(query, k)

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∑–∞–ø—Ä–æ—Å –≤ TF-IDF –≤–µ–∫—Ç–æ—Ä
            query_vec = self.vectorizer.transform([self._preprocess_text(query)])
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            top_indices = similarities.argsort()[-k:][::-1]
            
            cursor = self.connection.cursor(dictionary=True)
            similar_docs = []
            
            for idx in top_indices:
                if similarities[idx] > 0.1:  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
                    doc_id = self.document_ids[idx]
                    cursor.execute("SELECT content, source, type FROM documents WHERE id = %s", (doc_id,))
                    doc = cursor.fetchone()
                    if doc:
                        similar_docs.append({
                            'content': doc['content'],
                            'source': doc['source'],
                            'type': doc['type'],
                            'similarity': float(similarities[idx])
                        })
            
            cursor.close()
            
            if not similar_docs:
                return self._keyword_search(query, k)
                
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(similar_docs)}")
            return similar_docs

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return self._keyword_search(query, k)

    def _keyword_search(self, query, k=3):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        try:
            cursor = self.connection.cursor(dictionary=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            query_keywords = self._extract_keywords(query, top_n=5)
            keywords_list = query_keywords.split()
            
            if not keywords_list:
                return []
            
            # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º
            conditions = []
            params = []
            for keyword in keywords_list:
                conditions.append("(content LIKE %s OR keywords LIKE %s)")
                params.extend([f'%{keyword}%', f'%{keyword}%'])
            
            where_clause = " OR ".join(conditions)
            
            query_sql = f"""
            SELECT content, source, type
            FROM documents 
            WHERE {where_clause}
            LIMIT %s
            """
            
            params.append(k)
            cursor.execute(query_sql, params)
            docs = cursor.fetchall()
            
            formatted_docs = []
            for doc in docs:
                formatted_docs.append({
                    'content': doc['content'],
                    'source': doc['source'],
                    'type': doc['type'],
                    'similarity': 0.5  # –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∫–ª—é—á–µ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
                })
            
            cursor.close()
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {len(formatted_docs)}")
            return formatted_docs
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª—é—á–µ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def get_document_count(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            cursor = self.connection.cursor()
            cursor.execute("SELECT COUNT(*) FROM documents")
            count = cursor.fetchone()[0]
            cursor.close()
            return count
        except Error as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return 0

    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if self.connection:
            self.connection.close()
            logger.info("üîå –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å MySQL –∑–∞–∫—Ä—ã—Ç–æ")

def russian_stop_words():
    """–†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞"""
    return {
        '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞',
        '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ',
        '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞',
        '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ',
        '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π',
        '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º',
        '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç',
        '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º',
        '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞',
        '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å',
        '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è',
        '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥',
        '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ',
        '–≤—Å—é', '–º–µ–∂–¥—É'
    }