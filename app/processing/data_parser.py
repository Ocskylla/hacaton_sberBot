# app/processing/data_parser.py
import requests
from bs4 import BeautifulSoup
import re
import logging
from urllib.parse import urljoin

logger = logging.getLogger(__name__)


class DataParser:
    def __init__(self, base_url):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def clean_text(self, text):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤"""
        if not text:
            return ""
        
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r'[ \t]+', ' ', text)
        return text.strip()

    def extract_main_content(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
        selectors = [
            'main',
            'article',
            '.content',
            '.main-content',
            '.page-content',
            '#content',
            '#main',
            '.post-content',
            '.entry-content'
        ]
        
        for selector in selectors:
            content = soup.select_one(selector)
            if content:
                return content.get_text()
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä, –±–µ—Ä–µ–º body
        return soup.find('body').get_text() if soup.find('body') else soup.get_text()

    def parse_website(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å–∞–π—Ç–∞ –ª–∞–≥–µ—Ä—è"""
        pages_to_parse = [
            '/czto-kosmos', '/osnovnye-svedeniya', '/deyatelnost',
            '/fotogalereya/infrastruktura', '/profilnye-smeny', '/roditelyam',
            '/dostupnaya-sreda', '/oplata', '/struktura_i_organy',
            '/nashi-dostizheniya', '/muzej-czto-kosmos', '/fotogalereya/usloviya-prozhivaniya',
            '/dokumenty', '/kontakty'
        ]

        all_data = []

        for page in pages_to_parse:
            try:
                url = urljoin(self.base_url, page)
                logger.info(f"–ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
                
                response = self.session.get(url, timeout=15)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, 'html.parser')

                # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                for element in soup(["script", "style", "nav", "header", "footer"]):
                    element.decompose()

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title = soup.find('title')
                title_text = self.clean_text(title.get_text()) if title else ""

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
                main_text = self.extract_main_content(soup)
                cleaned_text = self.clean_text(main_text)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π
                if cleaned_text and len(cleaned_text) > 50:
                    content = f"{title_text}\n\n{cleaned_text}"
                    
                    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
                    if len(content) > 4000:
                        chunks = self.split_text(content, max_length=4000)
                        for i, chunk in enumerate(chunks):
                            all_data.append({
                                'source': url,
                                'content': chunk,
                                'type': 'website',
                                'chunk_index': i
                            })
                    else:
                        all_data.append({
                            'source': url,
                            'content': content[:5000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                            'type': 'website',
                            'chunk_index': 0
                        })
                    
                    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {url} (—Å–∏–º–≤–æ–ª–æ–≤: {len(cleaned_text)})")

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}: {e}")
                continue

        # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –ø—Ä–æ–±—É–µ–º –≥–ª–∞–≤–Ω—É—é
        if not all_data:
            try:
                logger.info("–ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
                response = self.session.get(self.base_url, timeout=15)
                soup = BeautifulSoup(response.text, 'html.parser')
                
                for element in soup(["script", "style", "nav", "header", "footer"]):
                    element.decompose()
                
                title = soup.find('title')
                title_text = self.clean_text(title.get_text()) if title else ""
                main_text = self.extract_main_content(soup)
                cleaned_text = self.clean_text(main_text)
                
                if cleaned_text:
                    all_data.append({
                        'source': self.base_url,
                        'content': f"{title_text}\n\n{cleaned_text}"[:5000],
                        'type': 'website',
                        'chunk_index': 0
                    })
                    logger.info(f"‚úÖ –†–∞—Å–ø–∞—Ä—Å–µ–Ω–∞ –≥–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
            except Exception as e:
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {e}")

        logger.info(f"üìä –í—Å–µ–≥–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(all_data)}")
        return all_data

    def split_text(self, text, max_length=4000):
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º"""
        sentences = re.split(r'[.!?]+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk) + len(sentence) + 1 <= max_length:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks

    def create_sample_faq(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—Ä–∞–∑—Ü–æ–≤ FAQ"""
        sample_faq = [
            {
                'question': '–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –∑–∞–µ–∑–¥–∞ –≤ –ª–∞–≥–µ—Ä—å?',
                'answer': '–î–ª—è –∑–∞–µ–∑–¥–∞ –≤ –ª–∞–≥–µ—Ä—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã: –ø–∞—Å–ø–æ—Ä—Ç —Ä–æ–¥–∏—Ç–µ–ª—è, —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ –æ —Ä–æ–∂–¥–µ–Ω–∏–∏ —Ä–µ–±–µ–Ω–∫–∞, –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è —Å–ø—Ä–∞–≤–∫–∞ —Ñ–æ—Ä–º—ã 079/—É, —Å–ø—Ä–∞–≤–∫–∞ –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ —Å –∏–Ω—Ñ–µ–∫—Ü–∏–æ–Ω–Ω—ã–º–∏ –±–æ–ª—å–Ω—ã–º–∏, –∫–æ–ø–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –ø–æ–ª–∏—Å–∞.',
                'type': 'faq'
            },
            {
                'question': '–ö–∞–∫–æ–≤–∞ —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—É—Ç–µ–≤–∫–∏?',
                'answer': '–°—Ç–æ–∏–º–æ—Å—Ç—å –ø—É—Ç–µ–≤–∫–∏ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–µ–∑–æ–Ω–∞ –∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã —Å–º–µ–Ω—ã. –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã —É—Ç–æ—á–Ω—è–π—Ç–µ —É –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏ –ª–∞–≥–µ—Ä—è –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É.',
                'type': 'faq'
            },
            {
                'question': '–ö–∞–∫–∏–µ –º–µ—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω—ã –≤ –ª–∞–≥–µ—Ä–µ?',
                'answer': '–õ–∞–≥–µ—Ä—å –æ–±–µ—Å–ø–µ—á–µ–Ω –∫—Ä—É–≥–ª–æ—Å—É—Ç–æ—á–Ω–æ–π –æ—Ö—Ä–∞–Ω–æ–π, –≤–∏–¥–µ–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ–º, –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–º –ø—É–Ω–∫—Ç–æ–º. –í—Å–µ –≤–æ–∂–∞—Ç—ã–µ –ø—Ä–æ—Ö–æ–¥—è—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–µ—Ç–µ–π.',
                'type': 'faq'
            },
            {
                'question': '–ö–∞–∫–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –≥—Ä—É–ø–ø—ã –ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è –≤ –ª–∞–≥–µ—Ä—å?',
                'answer': '–õ–∞–≥–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–µ—Ç–µ–π –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ –æ—Ç 7 –¥–æ 17 –ª–µ—Ç. –ì—Ä—É–ø–ø—ã —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.',
                'type': 'faq'
            },
            {
                'question': '–ï—Å—Ç—å –ª–∏ –≤ –ª–∞–≥–µ—Ä–µ Wi-Fi?',
                'answer': '–ù–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –ª–∞–≥–µ—Ä—è –µ—Å—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ Wi-Fi –¥–ª—è –¥–µ—Ç–µ–π –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ—Ç–≤–µ–¥–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è.',
                'type': 'faq'
            }
        ]

        documents = []
        for i, item in enumerate(sample_faq):
            content = f"–í–æ–ø—Ä–æ—Å: {item['question']}\n–û—Ç–≤–µ—Ç: {item['answer']}"
            documents.append({
                'source': 'sample_faq',
                'content': content,
                'type': 'faq',
                'chunk_index': i
            })

        logger.info(f"üìã –°–æ–∑–¥–∞–Ω–æ FAQ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
        return documents