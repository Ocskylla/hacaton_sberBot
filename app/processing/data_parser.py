# app/processing/data_parser.py
import requests
from bs4 import BeautifulSoup
import re
import logging
from urllib.parse import urljoin
import time

logger = logging.getLogger(__name__)


class DataParser:
    def __init__(self, base_url):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def clean_text(self, text):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤"""
        if not text:
            return ""
        
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r'[ \t]+', ' ', text)
        return text.strip()

    def extract_main_content(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
        selectors = [
            'main',
            'article',
            '.content',
            '.main-content',
            '.page-content',
            '#content',
            '#main',
            '.post-content',
            '.entry-content'
        ]
        
        for selector in selectors:
            content = soup.select_one(selector)
            if content:
                return content.get_text()
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä, –±–µ—Ä–µ–º body
        return soup.find('body').get_text() if soup.find('body') else soup.get_text()

    def parse_legal_documents(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å pravo.gov.ru"""
        legal_documents = []
        
        # –°–ø–∏—Å–æ–∫ –∑–∞–∫–æ–Ω–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
        laws_to_parse = [
            {
                'name': '–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω ‚Ññ124-–§–ó ¬´–û–± –æ—Å–Ω–æ–≤–Ω—ã—Ö –≥–∞—Ä–∞–Ω—Ç–∏—è—Ö –ø—Ä–∞–≤ —Ä–µ–±—ë–Ω–∫–∞¬ª',
                'article': '—Å—Ç. 12 —á. 2',
                'url': 'http://pravo.gov.ru/proxy/ips/?docbody=&nd=102058299'
            },
            {
                'name': '–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω ‚Ññ273-–§–ó ¬´–û–± –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ –≤ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏¬ª',
                'article': '—Å—Ç. 28 —á. 7',
                'url': 'http://pravo.gov.ru/proxy/ips/?docbody=&nd=102162277'
            },
            {
                'name': '–ó–∞–∫–æ–Ω –†–§ ‚Ññ2300-1 ¬´–û –∑–∞—â–∏—Ç–µ –ø—Ä–∞–≤ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π¬ª',
                'article': '—Å—Ç. 7 –ø. 1',
                'url': 'http://pravo.gov.ru/proxy/ips/?docbody=&nd=102030634'
            },
            {
                'name': '–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–§',
                'article': '—Å—Ç. 1068',
                'url': 'http://pravo.gov.ru/proxy/ips/?docbody=&nd=102450098'
            },
            {
                'name': '–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–§',
                'article': '—Å—Ç. 1095',
                'url': 'http://pravo.gov.ru/proxy/ips/?docbody=&nd=102450098'
            },
            {
                'name': '–£–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§',
                'article': '—Å—Ç. 293',
                'url': 'http://pravo.gov.ru/proxy/ips/?docbody=&nd=102450099'
            }
        ]

        for law in laws_to_parse:
            try:
                logger.info(f"–ü–∞—Ä—Å–∏–º –∑–∞–∫–æ–Ω: {law['name']} {law['article']}")
                
                response = self.session.get(law['url'], timeout=15)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, 'html.parser')

                # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                for element in soup(["script", "style", "nav", "header", "footer"]):
                    element.decompose()

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
                main_text = self.extract_main_content(soup)
                cleaned_text = self.clean_text(main_text)

                if cleaned_text and len(cleaned_text) > 50:
                    # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–∫–æ–Ω–∞
                    law_content = f"""
{law['name']} {law['article']}

–û–ü–ò–°–ê–ù–ò–ï:
–î–∞–Ω–Ω—ã–π –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π –∞–∫—Ç —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–µ—Ç—Å–∫–∏—Ö –ª–∞–≥–µ—Ä–µ–π –∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –∂–∏–∑–Ω—å –¥–µ—Ç–µ–π.

–¢–ï–ö–°–¢ –î–û–ö–£–ú–ï–ù–¢–ê:
{cleaned_text[:3000]}

–û–¢–í–ï–¢–°–¢–í–ï–ù–ù–û–°–¢–¨ –î–ï–¢–°–ö–û–ì–û –õ–ê–ì–ï–†–Ø:
- –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –ª–∞–≥–µ—Ä—è –Ω–µ—Å–µ—Ç –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ –∂–∏–∑–Ω—å –∏ –∑–¥–æ—Ä–æ–≤—å–µ –¥–µ—Ç–µ–π
- –û–±—è–∑–∞–Ω–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –ø—Ä–µ–±—ã–≤–∞–Ω–∏—è
- –û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –¥–µ–π—Å—Ç–≤–∏—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤
- –ù–µ—Å–µ—Ç –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–æ-–ø—Ä–∞–≤–æ–≤—É—é –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ –ø—Ä–∏—á–∏–Ω–µ–Ω–Ω—ã–π –≤—Ä–µ–¥
"""

                    legal_documents.append({
                        'source': law['url'],
                        'content': law_content,
                        'type': 'legal_document',
                        'chunk_index': 0,
                        'law_name': law['name'],
                        'article': law['article']
                    })
                    
                    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω –∑–∞–∫–æ–Ω: {law['name']} {law['article']}")

                time.sleep(2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∑–∞–∫–æ–Ω {law['name']}: {e}")
                continue

        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±–æ–±—â–∞—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –æ –ø—Ä–∞–≤–æ–≤–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
        responsibility_summary = """
–û–¢–í–ï–¢–°–¢–í–ï–ù–ù–û–°–¢–¨ –ê–î–ú–ò–ù–ò–°–¢–†–ê–¶–ò–ò –î–ï–¢–°–ö–ò–• –õ–ê–ì–ï–†–ï–ô –ü–û –ó–ê–ö–û–ù–û–î–ê–¢–ï–õ–¨–°–¢–í–£ –†–§

–ù–û–†–ú–ê–¢–ò–í–ù–´–ï –ü–†–ê–í–û–í–´–ï –ê–ö–¢–´, –†–ï–ì–£–õ–ò–†–£–Æ–©–ò–ï –û–¢–í–ï–¢–°–¢–í–ï–ù–ù–û–°–¢–¨:

1. "–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω ‚Ññ124-–§–ó ¬´–û–± –æ—Å–Ω–æ–≤–Ω—ã—Ö –≥–∞—Ä–∞–Ω—Ç–∏—è—Ö –ø—Ä–∞–≤ —Ä–µ–±—ë–Ω–∫–∞¬ª"
   - –°—Ç–∞—Ç—å—è 12 —á–∞—Å—Ç—å 2: –ó–∞–∫—Ä–µ–ø–ª—è–µ—Ç –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç—å –ª–∞–≥–µ—Ä—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∂–∏–∑–Ω–∏ –∏ –∑–¥–æ—Ä–æ–≤—å—è –¥–µ—Ç–µ–π.

2. "–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω ‚Ññ273-–§–ó ¬´–û–± –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ –≤ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏¬ª"
   - –°—Ç–∞—Ç—å—è 28 —á–∞—Å—Ç—å 7: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∑–∞ –∂–∏–∑–Ω—å –∏ –∑–¥–æ—Ä–æ–≤—å–µ –æ–±—É—á–∞—é—â–∏—Ö—Å—è.

3. "–ó–∞–∫–æ–Ω –†–§ ‚Ññ2300-1 ¬´–û –∑–∞—â–∏—Ç–µ –ø—Ä–∞–≤ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π¬ª"
   - –°—Ç–∞—Ç—å—è 7 –ø—É–Ω–∫—Ç 1: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–∞–≤–æ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —É—Å–ª—É–≥–∏.

4. "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–§"
   - –°—Ç–∞—Ç—å—è 1068: –õ–∞–≥–µ—Ä—å –Ω–µ—Å–µ—Ç –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ –Ω–µ–∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π –ø–æ –ø—Ä–∏—Å–º–æ—Ç—Ä—É –∑–∞ –¥–µ—Ç—å–º–∏.
   - –°—Ç–∞—Ç—å—è 1095: –õ–∞–≥–µ—Ä—å –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –≤—Ä–µ–¥, –ø—Ä–∏—á–∏–Ω–µ–Ω–Ω—ã–π –∑–¥–æ—Ä–æ–≤—å—é —Ä–µ–±–µ–Ω–∫–∞ –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –Ω–∞–¥–ª–µ–∂–∞—â–µ–≥–æ –ø—Ä–∏—Å–º–æ—Ç—Ä–∞.

5. "–£–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§"
   - –°—Ç–∞—Ç—å—è 293: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —É–≥–æ–ª–æ–≤–Ω—É—é –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ —Ö–∞–ª–∞—Ç–Ω–æ—Å—Ç—å –¥–æ–ª–∂–Ω–æ—Å—Ç–Ω—ã—Ö –ª–∏—Ü.

–í–ò–î–´ –û–¢–í–ï–¢–°–¢–í–ï–ù–ù–û–°–¢–ò:
- –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–æ-–ø—Ä–∞–≤–æ–≤–∞—è: –í–æ–∑–º–µ—â–µ–Ω–∏–µ –≤—Ä–µ–¥–∞, –ø—Ä–∏—á–∏–Ω–µ–Ω–Ω–æ–≥–æ –∂–∏–∑–Ω–∏ –∏ –∑–¥–æ—Ä–æ–≤—å—é
- –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–∞—è: –ù–∞—Ä—É—à–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –æ—Ç–¥—ã—Ö–∞ –¥–µ—Ç–µ–π
- –£–≥–æ–ª–æ–≤–Ω–∞—è: –•–∞–ª–∞—Ç–Ω–æ—Å—Ç—å, –ø–æ–≤–ª–µ–∫—à–∞—è —Ç—è–∂–∫–∏–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è

–ü–†–ê–í–ê –†–û–î–ò–¢–ï–õ–ï–ô:
- –¢—Ä–µ–±–æ–≤–∞—Ç—å –≤–æ–∑–º–µ—â–µ–Ω–∏—è –≤—Ä–µ–¥–∞, –ø—Ä–∏—á–∏–Ω–µ–Ω–Ω–æ–≥–æ —Ä–µ–±–µ–Ω–∫—É
- –û–±—Ä–∞—â–∞—Ç—å—Å—è –≤ –Ω–∞–¥–∑–æ—Ä–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã –ø—Ä–∏ –Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö
- –ü–æ–ª—É—á–∞—Ç—å –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É—Å–ª–æ–≤–∏—è—Ö –ø—Ä–µ–±—ã–≤–∞–Ω–∏—è
"""

        legal_documents.append({
            'source': 'legal_summary',
            'content': responsibility_summary,
            'type': 'legal_document',
            'chunk_index': 1
        })

        logger.info(f"üìö –í—Å–µ–≥–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(legal_documents)}")
        return legal_documents

    def parse_website(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å–∞–π—Ç–∞ –ª–∞–≥–µ—Ä—è"""
        pages_to_parse = [
            '/czto-kosmos', '/osnovnye-svedeniya', '/deyatelnost',
            '/fotogalereya/infrastruktura', '/profilnye-smeny', '/roditelyam',
            '/dostupnaya-sreda', '/oplata', '/struktura_i_organy',
            '/nashi-dostizheniya', '/muzej-czto-kosmos', '/fotogalereya/usloviya-prozhivaniya',
            '/dokumenty', '/kontakty'
        ]

        all_data = []

        for page in pages_to_parse:
            try:
                url = urljoin(self.base_url, page)
                logger.info(f"–ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
                
                response = self.session.get(url, timeout=15)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, 'html.parser')

                # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                for element in soup(["script", "style", "nav", "header", "footer"]):
                    element.decompose()

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title = soup.find('title')
                title_text = self.clean_text(title.get_text()) if title else ""

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
                main_text = self.extract_main_content(soup)
                cleaned_text = self.clean_text(main_text)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π
                if cleaned_text and len(cleaned_text) > 50:
                    content = f"{title_text}\n\n{cleaned_text}"
                    
                    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
                    if len(content) > 4000:
                        chunks = self.split_text(content, max_length=4000)
                        for i, chunk in enumerate(chunks):
                            all_data.append({
                                'source': url,
                                'content': chunk,
                                'type': 'website',
                                'chunk_index': i
                            })
                    else:
                        all_data.append({
                            'source': url,
                            'content': content[:5000],
                            'type': 'website',
                            'chunk_index': 0
                        })
                    
                    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {url} (—Å–∏–º–≤–æ–ª–æ–≤: {len(cleaned_text)})")

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}: {e}")
                continue

        # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –ø—Ä–æ–±—É–µ–º –≥–ª–∞–≤–Ω—É—é
        if not all_data:
            try:
                logger.info("–ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
                response = self.session.get(self.base_url, timeout=15)
                soup = BeautifulSoup(response.text, 'html.parser')
                
                for element in soup(["script", "style", "nav", "header", "footer"]):
                    element.decompose()
                
                title = soup.find('title')
                title_text = self.clean_text(title.get_text()) if title else ""
                main_text = self.extract_main_content(soup)
                cleaned_text = self.clean_text(main_text)
                
                if cleaned_text:
                    all_data.append({
                        'source': self.base_url,
                        'content': f"{title_text}\n\n{cleaned_text}"[:5000],
                        'type': 'website',
                        'chunk_index': 0
                    })
                    logger.info(f"‚úÖ –†–∞—Å–ø–∞—Ä—Å–µ–Ω–∞ –≥–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
            except Exception as e:
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {e}")

        # –î–æ–±–∞–≤–ª—è–µ–º —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        legal_docs = self.parse_legal_documents()
        all_data.extend(legal_docs)

        logger.info(f"üìä –í—Å–µ–≥–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(all_data)}")
        return all_data

    def split_text(self, text, max_length=4000):
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º"""
        sentences = re.split(r'[.!?]+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk) + len(sentence) + 1 <= max_length:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks

    def create_sample_faq(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—Ä–∞–∑—Ü–æ–≤ FAQ"""
        sample_faq = [
            {
                'question': '–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –∑–∞–µ–∑–¥–∞ –≤ –ª–∞–≥–µ—Ä—å?',
                'answer': '–î–ª—è –∑–∞–µ–∑–¥–∞ –≤ –ª–∞–≥–µ—Ä—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã: –ø–∞—Å–ø–æ—Ä—Ç —Ä–æ–¥–∏—Ç–µ–ª—è, —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ –æ —Ä–æ–∂–¥–µ–Ω–∏–∏ —Ä–µ–±–µ–Ω–∫–∞, –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è —Å–ø—Ä–∞–≤–∫–∞ —Ñ–æ—Ä–º—ã 079/—É, —Å–ø—Ä–∞–≤–∫–∞ –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ —Å –∏–Ω—Ñ–µ–∫—Ü–∏–æ–Ω–Ω—ã–º–∏ –±–æ–ª—å–Ω—ã–º–∏, –∫–æ–ø–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –ø–æ–ª–∏—Å–∞.',
                'type': 'faq'
            },
            {
                'question': '–ö–∞–∫–æ–≤–∞ —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—É—Ç–µ–≤–∫–∏?',
                'answer': '–°—Ç–æ–∏–º–æ—Å—Ç—å –ø—É—Ç–µ–≤–∫–∏ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–µ–∑–æ–Ω–∞ –∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã —Å–º–µ–Ω—ã. –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã —É—Ç–æ—á–Ω—è–π—Ç–µ —É –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏ –ª–∞–≥–µ—Ä—è –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É.',
                'type': 'faq'
            },
            {
                'question': '–ö–∞–∫–∏–µ –º–µ—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω—ã –≤ –ª–∞–≥–µ—Ä–µ?',
                'answer': '–õ–∞–≥–µ—Ä—å –æ–±–µ—Å–ø–µ—á–µ–Ω –∫—Ä—É–≥–ª–æ—Å—É—Ç–æ—á–Ω–æ–π –æ—Ö—Ä–∞–Ω–æ–π, –≤–∏–¥–µ–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ–º, –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–º –ø—É–Ω–∫—Ç–æ–º. –í—Å–µ –≤–æ–∂–∞—Ç—ã–µ –ø—Ä–æ—Ö–æ–¥—è—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–µ—Ç–µ–π.',
                'type': 'faq'
            },
            {
                'question': '–ö–∞–∫–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –≥—Ä—É–ø–ø—ã –ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è –≤ –ª–∞–≥–µ—Ä—å?',
                'answer': '–õ–∞–≥–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–µ—Ç–µ–π –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ –æ—Ç 7 –¥–æ 17 –ª–µ—Ç. –ì—Ä—É–ø–ø—ã —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.',
                'type': 'faq'
            },
            {
                'question': '–ï—Å—Ç—å –ª–∏ –≤ –ª–∞–≥–µ—Ä–µ Wi-Fi?',
                'answer': '–ù–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –ª–∞–≥–µ—Ä—è –µ—Å—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ Wi-Fi –¥–ª—è –¥–µ—Ç–µ–π –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ—Ç–≤–µ–¥–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è.',
                'type': 'faq'
            },
            {
                'question': '–ö–∞–∫–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏ –ª–∞–≥–µ—Ä—è –∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –¥–µ—Ç–µ–π?',
                'answer': '–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –ª–∞–≥–µ—Ä—è –Ω–µ—Å–µ—Ç –ø–æ–ª–Ω—É—é –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –¥–µ—Ç–µ–π —Å–æ–≥–ª–∞—Å–Ω–æ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É –†–§, –≤–∫–ª—é—á–∞—è –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω ‚Ññ124-–§–ó, –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–§ –∏ –¥—Ä—É–≥–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∞–∫—Ç—ã.',
                'type': 'faq'
            }
        ]

        documents = []
        for i, item in enumerate(sample_faq):
            content = f"–í–æ–ø—Ä–æ—Å: {item['question']}\n–û—Ç–≤–µ—Ç: {item['answer']}"
            documents.append({
                'source': 'sample_faq',
                'content': content,
                'type': 'faq',
                'chunk_index': i
            })

        logger.info(f"üìã –°–æ–∑–¥–∞–Ω–æ FAQ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
        return documents